{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annaskaroliny/latamAirlines/blob/main/%5BCONFIDENTIAL%5D_TW_LATAM_Allocation_Process_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LATAM Preparation - Complete Guide**\n",
        "\n",
        "**Welcome to the Data Engineer Guide.** On this occasion, you will have the opportunity to get closer to the reality of the role, demonstrate your skills and knowledge in data processing with Python and different data structures.\n",
        "\n",
        "This guide has all the tips and recommendations to succesfully crack all the steps required for LATAM allocation process. It is a reflection of the preparation and strategy that I took to embrace and crack challenges for LATAM. Enjoy!!\n",
        "\n",
        "**This document is for TWERS use only and it contains confidential information.**\n",
        "\n"
      ],
      "metadata": {
        "id": "nbP1386ud-Vx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hackerrank Test preparation**\n",
        "\n",
        "My approach to crack the Hackerrank challenge was based on the Topics that are covered, using Gemini and the tips and recommendations I could get from other TWERS.\n",
        "\n",
        "* The HackerRank test consists of 23 questions in total. 20 of unique and multiple selection. And 3 python coding questions.\n",
        "* The test topics are described in the last section of the Gemini input.\n",
        "* This is the input I created for him (consider that if the answer is too large, you will need to ask Gemini to continue the explanation) - Please don't use it only but read it as well because it has everything I could get in order to be prepared:\n",
        "\n",
        "\n",
        "```\n",
        "I have to prepare for a hackerrank test and that there are questions that can be general theory and not only related to GCP.\n",
        "It will be cool to have both views.\n",
        "\n",
        "I know that they ask for monolite, legacy, soa, design patterns, google api, software engineering patterns, architecture.\n",
        "As far as I know the questions can be a definition or well giving you context to see what is best in that question.\n",
        "Yet, that is what I know and there can be another questions as well.\n",
        "\n",
        "I need as much as possible business case questions and answers (related to airlines and other domains) as well as concept questions and answers, all based on hackerrank preferently, within the list of topics that I will give to you.\n",
        "I want all the questions and answers you can find.\n",
        "I have time to read everything so it doesn't matter if the answer is large.\n",
        "\n",
        "There are 20 questions of unique and multiple selection and I have 1.5 hours to solve them, from the following list with the format \"Topic: What we waiting for?\" -- please give and explain everything you have, for example if you suggest something, explain what it is or how to do it, in other words, don't make me going to another webpages nor asking you things back, don't suggest doing things, do them yourself. If something requires an example you give it.\n",
        "If code is needed use python and if you need sql use sql.\n",
        "\n",
        "Consider that I don't know anything related to this so if you use a professional term, you have to explain it and give examples if necessary:\n",
        "- GIT (1 question): Good versioning practices, ability to work as a team with distributed responsibilities, maintaining orderly and controlled code\n",
        "- Data structure & algorithms (4 questions): How data is organized and stored efficiently & Step-by-step methods for solving problems efficiently.\n",
        "- CI/CD (2 questions): Experience in deployment and integration platforms within the development cycle\n",
        "- IO (1 question): Manipulation of process input and output data optimally\n",
        "- Cloud (3 questions): Experience in cloud development and resource integration\n",
        "- Design and architecture patterns (5 questions): Ability to recognize/implement patterns to solve common problems in software development\n",
        "- Programming paradigms and development strategies (4 questions): Principles or styles for creating software and some common strategies for development\n",
        "```\n",
        "\n",
        "* TIP: For python development questions (3) you can exercise in Hackerrank: arrays, HTML parsing (investigate BeautifulSoup) and str."
      ],
      "metadata": {
        "id": "hIy7-01aeWdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Ingest, Storage and Processing with Google Drive, Google Cloud Storage and Google BigQuery in Google Colab (Jupyter) with Python 3.10**\n",
        "\n",
        "**Preparation:**\n",
        "\n",
        "1. Initial project commit (done with GitHub desktop)\n",
        "2. Install Git Flow with `brew install git-flow`\n",
        "3. Configure the repository with `git flow init`\n",
        "4. Configure feature finishes to be done only in develop with `git config gitflow.feature.finish.keepremote true`\n",
        "5. Configure releases to not directly merge on main with `git config gitflow.feature.finish.keepremote=true`\n",
        "6. Synchronize the repository with GDrive\n",
        "7. Read the code from GDrive with Colab\n",
        "\n",
        "**Additional Notes:**\n",
        "\n",
        "* The `README.md` file mentions using GitHub Desktop, Git Flow, and Colab. These are tools that can be used for version control and code collaboration.\n",
        "* The `README.md` file also mentions measuring time and memory. This can be done using Python's built-in `time` and `memory_profiler` modules.\n",
        "* English was used for both documentation and code."
      ],
      "metadata": {
        "id": "JjRrIKcVec68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Challenge Guidelines:**\n",
        "\n",
        "**Repository:**\n",
        "\n",
        "* Your solution must be in a public repository on the GitHub platform.\n",
        "\n",
        "**Submitting your challenge:**\n",
        "\n",
        "1. Make a POST request to [https://advana-challenge-check-api-cr-k4hdbggvoq-uc.a.run.app/data-engineer](https://advana-challenge-check-api-cr-k4hdbggvoq-uc.a.run.app/data-engineer).\n",
        "2. The request body should be a JSON object with the following fields:\n",
        "    * `name`: Your full name\n",
        "    * `mail`: Your email address\n",
        "    * `github_url`: The URL of your GitHub repository containing the solution\n",
        "\n",
        "**Deadline:**\n",
        "\n",
        "* The deadline for submitting the challenge is 5 calendar days after receiving the challenge.\n",
        "\n",
        "**Technology and Techniques:**\n",
        "\n",
        "* You can use any technology or technique you prefer for data processing.\n",
        "* We will value your knowledge of cloud platforms.\n",
        "* If you use cloud platforms, follow the steps in your files WITHOUT adding access credentials to the different services.\n",
        "\n",
        "**Ranking Criteria:**\n",
        "\n",
        "* Challenges that are clearly organized, explanatory, modular, efficient, and creative will be ranked higher.\n",
        "\n",
        "**Assumptions and Documentation:**\n",
        "\n",
        "* Write down the assumptions you are making.\n",
        "* Include the versions of the libraries you are using in the requirements.txt file.\n",
        "* Do not delete what is already written in the requirements.txt file.\n",
        "* For this challenge, we recommend that you clearly describe how each part of your exercise can be improved.\n",
        "\n",
        "**Data:**\n",
        "\n",
        "* You must use the data contained in the provided file.\n",
        "* You can use the official Twitter documentation to understand the data structure.\n",
        "\n",
        "**Git Usage:**\n",
        "\n",
        "* We will positively evaluate good practices of Git usage.\n",
        "* Use the main branch for any final version you want us to review.\n",
        "* We recommend that you use some GitFlow practice.\n",
        "* Do not delete your development branches.\n",
        "\n",
        "**Error Handling and Edge Cases:**\n",
        "\n",
        "* Consider error handling and edge cases.\n",
        "\n",
        "**Maintainability, Readability, and Scalability:**\n",
        "\n",
        "* Remember that you will be working with other developers, so the maintainability, readability, and scalability of your code is essential.\n",
        "\n",
        "**Code Documentation:**\n",
        "\n",
        "* Good code documentation always helps the reader.\n",
        "\n",
        "**Additional Notes:**\n",
        "\n",
        "* The `README.md` file mentions using GitHub Desktop, Git Flow, and Colab. These are tools that can be used for version control and code collaboration.\n",
        "* The `README.md` file also mentions measuring time and memory. This can be done using Python's built-in `time` and `memory_profiler` modules.\n",
        "* `# type: ignore` was used for thois libraries that exists within the google colab environment by default and they are not required in the `requirements.txt` file"
      ],
      "metadata": {
        "id": "ywW-lV1XiA4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Beginning of Challenge Code**\n",
        "My approach was to firstly have a snippet with all the imports required for the entire notebook. Remember that if you want to load python files in your project you can use the magic command `%run file.py`.\n",
        "\n",
        "Remember to always comment or document your assumptions so others can understand the way you think things!!\n",
        "\n",
        "Here is an example of initial imports:\n",
        "\n",
        "```\n",
        "# Import the 'sys' library for interacting with the interpreter\n",
        "import sys\n",
        "\n",
        "# Exit the script if it's not the main program (avoids unintended execution as a module)\n",
        "if __name__ != \"__main__\":\n",
        "    sys.exit()\n",
        "\n",
        "# Import general-purpose libraries:\n",
        "import logging  # For logging messages\n",
        "import os  # For interacting with the operating system\n",
        "import time  # For working with time-related functions\n",
        "\n",
        "# Import type-related libraries:\n",
        "from typing import List, Tuple  # For type annotations\n",
        "import datetime  # For working with dates and times\n",
        "\n",
        "# Import I/O-related libraries:\n",
        "import io  # For common I/O operations\n",
        "\n",
        "# Import Google Colab-specific libraries:\n",
        "from google.colab import drive  # For interacting with Google Drive\n",
        "\n",
        "# Print a message indicating successful library imports\n",
        "print(\"All initial libraries were imported\")\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "JBWfZZHPiEXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Definitions and Configurations:**\n",
        "This code snippet defines constants used in the data transfer and processing pipeline:\n",
        "\n",
        "- **Google Cloud Storage (GCS) Information:**\n",
        "    - `BUCKET_NAME`: Specifies the name of the GCS bucket where data will be uploaded (`tw-gcp-public-lab`).\n",
        "    - `FOLDER_NAME`: Denotes the folder within the bucket to store the uploaded file (`raw`).\n",
        "    - `ZIP_FILE_NAME`: Represents the name of the compressed file containing tweets data (`tweets.json.zip`).\n",
        "    - `GCS_SOURCE_URI`: Constructs the full URI for the file location in GCS after upload (`gs://tw-gcp-public-lab/raw/`).\n",
        "\n",
        "- **Local File Paths:**\n",
        "    - `SOURCE_PATH`: Currently defines a local file path (`/content/drive/Othercomputers/My Mac/latam-challenge`), but it's not used in the provided code for downloading.\n",
        "\n",
        "- **Google Cloud Project and Dataset Information:**\n",
        "    - `PROJECT_ID`: Specifies the Google Cloud project ID (`tw-techdash`).\n",
        "    - `DATASET_NAME`: Defines the name of the BigQuery dataset where the data will be loaded (`tweets_dataset`).\n",
        "    - `TABLE_NAME`: Identifies the name of the BigQuery table to store the extracted tweets data (`tweets`).\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- The `SOURCE_PATH` might require modification if you intend to download a file from a different location.\n",
        "- Consider using environment variables or a configuration file to manage these constants, making your code more flexible and easier to maintain."
      ],
      "metadata": {
        "id": "tbtlOlnRiV_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Jupyter Kernel code reloading**\n",
        "\n",
        "**Functionality:**\n",
        "\n",
        "* This code snippet utilizes magic commands within Jupyter Notebooks to manage code reloading.\n",
        "* The `%reload_ext autoreload` line imports and activates the `autoreload` extension.\n",
        "* The `%autoreload 2` line configures the `autoreload` extension to automatically reload Python modules when changes are detected.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "* **Jupyter Magic Commands:** `%` prefix is used for magic commands that provide special functionality within Jupyter notebooks.\n",
        "* **Autoreload Extension:**  A Jupyter extension that automatically reloads Python modules when changes are detected in the corresponding source files.\n",
        "* **Reload Level:** The level `2` specifies that reload should occur when source files or any imported modules are modified (level 1 only reloads source file changes).\n",
        "\n",
        "**Overall Assessment:**\n",
        "\n",
        "* This code improves development efficiency within Jupyter notebooks by automatically reloading code, avoiding manual restarts.\n",
        "* It leverages the `autoreload` extension for automatic reloading functionality.\n",
        "* The configuration level `2` ensures comprehensive reloading behavior.\n",
        "\n",
        "**Potential Enhancements:**\n",
        "\n",
        "* While automatic reloading is helpful in development, it might not be suitable for production environments due to potential unexpected behavior during execution.\n",
        "* Consider using this approach primarily for interactive development within Jupyter notebooks."
      ],
      "metadata": {
        "id": "Nd5ayAAriZB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Google Drive mounting**\n",
        "\n",
        "**Functionality:**\n",
        "\n",
        "1. **Connects Google Drive:** This code establishes a connection between your Google Drive storage and the virtual machine running the Colab notebook.\n",
        "2. **Navigates to Project Directory:** This magic command changes the working directory within the Colab notebook to a specific location within your project directory.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "* **Google Drive Mounting:**\n",
        "    - `from google.colab import drive`: Imports the `drive` module for interacting with Google Drive from Colab.\n",
        "    - `drive.mount('/content/drive', force_remount=True)`: Mounts your Drive at the `/content/drive` path within Colab.\n",
        "    - **Authorization:** Requires initial authorization to grant Colab access to your Drive.\n",
        "* **Jupyter Notebook Magic Commands:**\n",
        "    - `%cd`: A magic command specifically designed for changing directories.\n",
        "\n",
        "**Overall Assessment:**\n",
        "\n",
        "* **Convenient Data Access:** Enables seamless access to your personal data stored in Google Drive for use within Colab notebooks.\n",
        "* **Improved Code Organization:** Helps organize your notebook within the project structure by focusing on a specific subdirectory (like \"src\").\n",
        "\n",
        "**Potential Enhancements:**\n",
        "\n",
        "* **Google Drive Mounting:**\n",
        "    - **Error Handling:** Consider incorporating `try-except` blocks to gracefully handle potential mounting issues.\n",
        "    - **Authentication Persistence:** Explore ways to persist the authentication token (if applicable) to avoid re-authorization for every session.\n",
        "* **Navigation:**\n",
        "    - **Clear Path Definitions:** Replace `{SOURCE_PATH}` with the actual path to your project directory for clarity.\n",
        "    - **Error Handling:** Consider handling potential issues like non-existent directories using Python code (like `try-except` blocks).\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "1. **Mount Google Drive:** The first part of the code imports the `drive` module and mounts your Google Drive to the `/content/drive` directory within Colab. This allows you to access your Drive files from within your notebook.\n",
        "2. **Change Directory:** The `%cd {SOURCE_PATH}/src` line uses a magic command to navigate to the subdirectory named \"src\" within your project directory (assuming `{SOURCE_PATH}` points to the correct location). This helps organize your notebook by focusing on the relevant project code.\n",
        "\n",
        "**Important Notes:**\n",
        "\n",
        "* Replace `{SOURCE_PATH}` with the actual path to your project directory on your machine.\n",
        "* You'll need to go through an authorization process the first time you run the mounting code to grant Colab access to your Drive.\n",
        "* It was not possible to have this"
      ],
      "metadata": {
        "id": "Q9EmXjC1ie9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Libraries requirements and imports**\n",
        "\n",
        "**Functionality:**\n",
        "\n",
        "- **Installs Python Libraries:** This code snippet installs a collection of Python libraries listed in a file named `requirements.txt` within the currently active virtual environment.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "- **requirements.txt File:** This text file contains a list of library names and their version requirements, ensuring consistent installation across environments.\n",
        "- **Virtual Environments:** Virtual environments isolate project dependencies, preventing conflicts with other Python projects on your system.\n",
        "- **sys.executable:** This Python variable points to the path of the Python interpreter for the active virtual environment.\n",
        "- **pip:** The Python Package Installer (pip) is used for managing Python packages and libraries.\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "1. **`import sys`**: Imports the `sys` module, providing access to system-specific variables and functions.\n",
        "2. **`!{sys.executable} -m pip install -r '../requirements.txt'`**: This line calls the pip installer within the virtual environment:\n",
        "   - **`!`**: Jupyter Notebook magic command to execute terminal commands.\n",
        "   - **`{sys.executable}`**: Ensures pip is called from the virtual environment's Python interpreter.\n",
        "   - **`-m`**: Designates a module to execute as a script (in this case, `pip`).\n",
        "   - **`install -r`**: Instructs pip to install packages from a requirements file.\n",
        "   - **`'../requirements.txt'`**: Specifies the path to the requirements file (relative to the current notebook's directory).\n",
        "\n",
        "**Important Notes:**\n",
        "\n",
        "- **Virtual Environment Activation:** Ensure you've activated the desired virtual environment before running this code.\n",
        "- **Path to requirements.txt:** Verify that `../requirements.txt` correctly points to the file's location.\n",
        "- **Internet Connection:** An internet connection is required for pip to download and install packages.\n",
        "\n",
        "**Overall Assessment:**\n",
        "\n",
        "- **Efficient Dependency Management:** Using `requirements.txt` is a best practice for managing project dependencies consistently.\n",
        "- **Consistent Environments:** Facilitates consistent library installations across different machines for reproducibility.\n",
        "- **Collaboration:** Enables easy setup of the same project environment for others.\n",
        "\n",
        "**Potential Enhancements:**\n",
        "\n",
        "- **Error Handling:** Consider incorporating error handling (like try-except blocks) to gracefully handle potential issues during installation, such as network connectivity problems or missing packages.\n"
      ],
      "metadata": {
        "id": "AAI1hYkVihgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ingest Google Drive ZIP into Google Storage**\n",
        "**Key Functions:**\n",
        "\n",
        "1. **authenticate_google_drive()**: Authenticates with Google Drive using the user's credentials.\n",
        "2. **download_file_from_drive(drive_service, file_id)**: Downloads a specified file from Google Drive.\n",
        "3. **upload_file_to_cloud_storage(client, bucket_name, folder_name, downloaded, zip_file_name)**: Uploads a file to Google Cloud Storage, handling folder creation if needed.\n",
        "4. **decompress_zip_file(client, bucket_name, folder_name, zip_file_name)**: Decompresses a ZIP file within a GCS bucket.\n",
        "\n",
        "**Code Structure:**\n",
        "\n",
        "- **Logging:** Employs `logging` for debugging and tracking progress.\n",
        "- **Error Handling:** Uses try-except blocks to gracefully handle potential errors.\n",
        "- **Modularity:** Separates functionality into distinct, reusable functions.\n",
        "- **Type Hints:** Enhances code readability and potential type checking.\n",
        "\n",
        "**Main Code Execution:**\n",
        "\n",
        "1. Configures logging to a file named 'transfer.log'.\n",
        "2. Authenticates with Google Drive.\n",
        "3. Downloads the specified file from Drive.\n",
        "4. Creates a Cloud Storage client.\n",
        "5. Uploads the downloaded file to GCS.\n",
        "6. Decompresses the ZIP file in GCS if its content type is 'application/zip'.\n",
        "7. Logs success or failure messages.\n",
        "8. Finally, ensures the downloaded file is closed.\n",
        "\n",
        "**Overall Assessment:**\n",
        "\n",
        "- **Well-structured:** The code is organized, modular, and includes error handling.\n",
        "- **Clear Functionality:** It effectively handles file transfer and decompression tasks.\n",
        "- **Authentication Flexibility:** Uses authentication methods external to the code (useful for avoiding credentials in code).\n",
        "- **Good Practices:** Adheres to good practices like logging and try-except blocks.\n",
        "\n",
        "**Potential Enhancements:**\n",
        "\n",
        "- **Parameterization:** Explore using command-line arguments or configuration files to adjust parameters more flexibly.\n",
        "- **Progress Reporting:** Consider more granular progress reporting for downloads/uploads.\n",
        "- **Content Validation:** Validate file content after decompression for integrity.\n",
        "- **Advanced Error Handling:** Implement retries or alternative actions for potential errors.\n",
        "\n",
        "This code provides a foundation for file transfer and decompression tasks within Google Cloud environments, demonstrating clarity and attention to best practices."
      ],
      "metadata": {
        "id": "ch-4ShX8ikh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BigQuery Storage Functions**\n",
        "\n",
        "**Functionality:**\n",
        "\n",
        "These Python functions interact with BigQuery to authenticate, create datasets and tables, and load data from Cloud Storage.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "* **Client:** The `bigquery.Client` object is central to interacting with BigQuery.\n",
        "* **Datasets and Tables:** Datasets organize tables, and both can be created or overwritten using these functions.\n",
        "* **Data Loading:** Data is loaded from Cloud Storage in newline-delimited JSON format, and BigQuery automatically infers the schema.\n",
        "* **Error Handling:** The functions use logging and try-except blocks to handle errors and provide informative messages.\n",
        "\n",
        "**Potential Enhancements:**\n",
        "\n",
        "* **Progress Reporting:** Consider logging loading progress.\n",
        "* **Data Validation:** Explore adding data validation checks before loading.\n"
      ],
      "metadata": {
        "id": "NPD4ZQZkjTKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BigQuery Processing Functions**\n",
        "\n",
        "**Functionality:**\n",
        "\n",
        "- **Processes BigQuery Results:** The `process_bigquery_results` function executes the query, handles results, and converts them into a desired format (list of tuples with date and username).\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "- **Type Hints:** Employs type hints (`List`, `Tuple`, `datetime.date`) for improved code readability and potential static type checking.\n",
        "- **Error Handling:** Incorporates `try-except` blocks to gracefully handle exceptions (`BadRequest` and generic exceptions).\n",
        "- **Data Conversion:** Converts retrieved data rows into the specified format.\n",
        "\n",
        "**Overall Assessment:**\n",
        "\n",
        "- **Clear Separation:** Functions promote modularity and reusability.\n",
        "- **Meaningful Variable Names:** Descriptive names enhance code understandability.\n",
        "- **Error Management:** Handles potential errors during query execution and processing.\n",
        "\n",
        "**Potential Enhancements:**\n",
        "\n",
        "- **Input Validation:** Consider validating the constructed query string before execution.\n",
        "- **Logging:** Integrate logging for detailed tracking and debugging.\n",
        "- **Security:** Ensure secure credential management for BigQuery access.\n",
        "- **Query Parameterization:** If DATASET_NAME and TABLE_NAME are not intended for hardcoding, utilize BigQuery's query parameters for better reusability and security.\n",
        "- **Data Usage:** Currently, the extracted data is printed. You can modify this section to store the data in a desired location or perform further processing.\n",
        "\n",
        "This code provides a foundation for working with BigQuery data retrieval and processing. You can extend it based on your specific needs."
      ],
      "metadata": {
        "id": "Yt7yIC7JkO5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BigQuery Queries**\n",
        "\n",
        "**Snippet 1: Top 10 Dates with Top Users**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
        " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
        " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
        " (datetime.date(2021, 2, 16), 'jot__b'),\n",
        " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
        " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
        " (datetime.date(2021, 2, 15), 'jot__b'),\n",
        " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
        " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
        " (datetime.date(2021, 2, 19), 'Preetm91')]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**Functionality**\n",
        "\n",
        "This SQL query identifies the top 10 dates with the most tweets and, for each of those dates, finds the user with the most tweets (considering usernames alphabetically in case of ties).\n",
        "\n",
        "**Key Concepts**\n",
        "\n",
        "* **Common Table Expressions (CTEs):** The query utilizes two CTEs:\n",
        "    * `TopDates`: Calculates the daily tweet count and ranks them in descending order, selecting the top 10.\n",
        "    * `TopUsersDate`: Joins the `tweets` table with `TopDates` to find the user(s) with the most tweets for each top date. It uses `ROW_NUMBER()` to handle ties by username order.\n",
        "* **Window Functions:** `ROW_NUMBER()` is used within `TopUsersDate` to assign a unique row number within each date partition, ordered by tweet count (descending) and then by number of tweets per user (descending).\n",
        "* **Filtering:** The final result retrieves users with `row_number = 1` (the user with the most tweets for each date).\n",
        "\n",
        "**Overall Assessment**\n",
        "\n",
        "This query effectively addresses the task by leveraging CTEs for modularity and window functions to handle ranking and ties.\n",
        "\n",
        "**Potential Enhancements**\n",
        "\n",
        "* **Clarity:** Consider adding comments within the query to explain the purpose of each CTE.\n",
        "* **Efficiency:** Explore alternative approaches to handle ties if performance is critical.\n",
        "\n",
        "**Data Usage**\n",
        "\n",
        "The query currently prints the `tweets_date` and `username`. You might want to consider storing this information in a table or using it for further analysis.\n",
        "\n",
        "**Snippet 2: Top 10 Most Used Emojis**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "[('ðŸ™', 7286),\n",
        "('ðŸ˜‚', 3072),\n",
        "('ðŸšœ', 2972),\n",
        "('âœŠ', 2411),\n",
        "('ðŸŒ¾', 2363),\n",
        "('ðŸ‡®', 2096),\n",
        "('ðŸ‡³', 2094),\n",
        "('â¤', 1779),\n",
        "('ðŸ‘‡', 1108),\n",
        "('ðŸ’š', 1040)]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**Functionality**\n",
        "\n",
        "This query extracts emojis from tweets and identifies the top 10 most frequently used emojis along with their counts.\n",
        "\n",
        "**Key Concepts**\n",
        "\n",
        "* **Regular Expressions (RegEx):** The `REGEXP_REPLACE()` function utilizes a complex RegEx pattern to capture a wide range of emoji characters across different Unicode blocks.\n",
        "* **UNNEST:** The `UNNEST()` operator is used to explode the extracted emoji list into a single row per emoji for counting.\n",
        "\n",
        "**Overall Assessment**\n",
        "\n",
        "This query effectively extracts and counts emojis, providing valuable insights into emoji usage.\n",
        "\n",
        "**Potential Enhancements**\n",
        "\n",
        "* **Filtering:** Depending on the analysis goals, you might want to filter out specific emoji categories (e.g., flags, country codes).\n",
        "* **Normalization:** Consider normalizing emojis to a canonical form to handle variations (e.g., skin tone modifiers).\n",
        "\n",
        "**Data Usage**\n",
        "\n",
        "The query currently prints the `emoji` and `count`. You could store this information for further analysis of emoji popularity.\n",
        "\n",
        "**Snippet 3: Top 10 Influential Users**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "[('narendramodi', 2265),\n",
        " ('Kisanektamorcha', 1840),\n",
        " ('RakeshTikaitBKU', 1644),\n",
        " ('PMOIndia', 1427),\n",
        " ('RahulGandhi', 1146),\n",
        " ('GretaThunberg', 1048),\n",
        " ('RaviSinghKA', 1019),\n",
        " ('rihanna', 986),\n",
        " ('UNHumanRights', 962),\n",
        " ('meenaharris', 926)]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**Functionality**\n",
        "\n",
        "This query identifies the top 10 users with the most mentions (`@username`) received in tweets.\n",
        "\n",
        "**Key Concepts**\n",
        "\n",
        "* **UNNEST:** Similar to snippet 2, `UNNEST()` is used to explode the mentioned user list from each tweet for counting mentions.\n",
        "\n",
        "**Overall Assessment**\n",
        "\n",
        "This query effectively identifies influential users based on mentions.\n",
        "\n",
        "**Potential Enhancements**\n",
        "\n",
        "* **Filtering:** You might consider filtering out self-mentions or mentions from specific accounts.\n",
        "* **Weighted Mentions:** Depending on the analysis goals, explore assigning weights to mentions based on factors like follower count.\n",
        "\n",
        "**Data Usage**\n",
        "\n",
        "The query currently prints the `username` and `mention_count`. You could store this information for further analysis of user influence."
      ],
      "metadata": {
        "id": "SOjOeCIikY9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wn6IVIzWmKfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Good Luck!!!**\n"
      ],
      "metadata": {
        "id": "3Uh9nX9QkiZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Notes**\n",
        "\n",
        "### Reflection and Continuous Improvement\n",
        "\n",
        "**Release:**\n",
        "\n",
        "* \"Release\" is used for final testing before releasing a version to \"main\". I used it 3 times at the beginning.\n",
        "* Gitflow makes using Git easier by saving commands.\n",
        "* **Be careful with the commit text:**\n",
        "    * The \"ammend\" command generates a \"force push\" and 2) does not work as it should.\n",
        "    * Make sure the branch is not deleted in the PR.\n",
        "    * Gitflow deletes it by default when closing the flow. I configured it not to.\n",
        "    * Renaming a branch also deletes it.\n",
        "* **Always do \"pull\" before working!**\n",
        "* **It is better to make a pull request for a single commit.**\n",
        "* At first I used Gitflow practices without Gitflow, but then I started using it completely.\n",
        "* Gitflow is simpler to use, but the branch naming conventions were difficult for me (e.g. \"feature_ignore\" vs \"feature/ignore\").\n",
        "* **Releases generate a \"release tag\" and homologate both \"develop\" and \"main\".**\n",
        "* I hesitated between using English or Spanish, but I understand that English is better for universality, consistency, conventions and compatibility with tracking tools.\n",
        "* I had to figure out how to respond to both Gitflow and pull requests. I mixed Gitflow, Github Desktop, Github's graphical interface and Visual Studio Code to achieve this.\n",
        "* It took me time to achieve a uniform and constant rhythm through the branches and progress. I learned to start all features first, then upload all PRs and finally approve them all. I think this would work well if I'm working in a team, as I've checked that no errors occur.\n",
        "* I experienced minor divergence conflicts between the origin and local branches, this is avoided by following the steps learned throughout this challenge.\n",
        "* **Hotfix is dangerous because it makes changes directly to \"main\". I tried it and prefer to avoid it.**\n",
        "* I experienced a major conflict with merge mixes in different branches due to not following a logical order of steps when there are multiple local changes. For the sake of time, I had to forcefully solve it by approving a pull request that authorized the emergency merge. This was similar to a hotfix but since I didn't understand the concept well. When I detected this situation I had to make a backup immediately, as well as save the files that Visual Studio Code keeps temporarily even if they are deleted. Finally I solved it and renamed the branches with â€œrevertâ€ and â€œmergeâ€ to follow the hotfix nomenclature.\n",
        "\n",
        "**Thoughts on Scala:**\n",
        "\n",
        "* It is important to handle inputs and outputs correctly, so I used type annotations to control IOs mostly.\n",
        "* Documentation, clean code, assumptions, error handling, edge cases and tests. I gave Gemini the context in the readme to generate assessments and documentation of my code.\n",
        "\n",
        "**Mistakes:**\n",
        "\n",
        "* Initially I focused on getting the code working and then organizing it into separate files. This was a mistake. I should have done it from the start, as it improves organization, readability, maintenance, modularity and benefits teamwork.\n",
        "* **Learning:**\n",
        "    * I still have a lot to learn and if I do well, I should make sure to absorb all the GIT practices I can learn from the experience with other people.\n",
        "    * This challenge was a great challenge for me, as I had no experience with GCP. My strength is Spark and Databricks, which is much simpler to use. However, I focused on achieving this challenge with Google Cloud Storage and BigQuery.\n",
        "    * In the end, I ran everything to verify that it works well. However, and therefore, I didn't realize that when I commented my code using Gemini's help, it changed the outputs of some of my definitions. I was always careful because I noticed it several times. In fact, I asked it not to do it. But I must be more careful and do thorough reviews before moving on to final steps in the main branches. I just checked everything focusing on outputs to check that everything is working well.\n",
        "\n",
        "### Configurations\n",
        "\n",
        "* **To only merge in \"dev\" in releases and thus go through pull request in \"main\":**\n",
        "\n",
        "```\n",
        "git config gitflow.release.finish.merge 0\n",
        "```\n",
        "\n",
        "* **To prevent development branches from being deleted on the origin:**\n",
        "\n",
        "```\n",
        "git config gitflow.feature.finish.keepremote=true\n",
        "git config --global gitflow.release.keepremote true\n",
        "```\n",
        "\n",
        "### Features\n",
        "\n",
        "* **To start a feature:**\n",
        "\n",
        "```\n",
        "git flow feature start feature\n",
        "git checkout -b feature/\n",
        "```\n",
        "\n",
        "* **Commits and PR in \"develop\" branch:**\n",
        "\n",
        "```\n",
        "feat: â€¦\n",
        "Detail\n",
        "```\n",
        "\n",
        "* **To finish a feature:**\n",
        "\n",
        "```\n",
        "git flow feature finish feature\n",
        "restore branch in PR\n",
        "```\n",
        "\n",
        "### Releases (caution)\n",
        "\n",
        "* **Full preventive pull:**\n",
        "\n",
        "```\n",
        "git pull --all\n",
        "```\n",
        "\n",
        "* **To start a release:**\n",
        "\n",
        "```\n",
        "git flow release start v0.0.\n",
        "git push origin release/v0.0.\n",
        "git log --oneline > changelog.txt\n",
        "```\n",
        "\n",
        "* **In Github:**\n",
        "\n",
        "```\n",
        "rel: â€¦\n",
        "Detail\n",
        "```\n",
        "\n",
        "* **PR in \"develop\" branch:**\n",
        "\n",
        "```\n",
        "git flow release finish 'v0.0.2' (this generates the tag)\n",
        "tag description: â€œVersion 0.0.2\n",
        "```\n",
        "\n",
        "### Hotfixes (caution)\n",
        "\n",
        "* **To start a hotfix:**\n",
        "\n",
        "```\n",
        "git flow hotfix start reload\n",
        "```\n",
        "\n",
        "* **Commits and PR in \"main\" branch:**\n",
        "\n",
        "```\n",
        "hotfix: Fixâ€¦\n",
        "Detail\n",
        "```\n",
        "\n",
        "* **To finish a hotfix:**\n",
        "\n",
        "```\n",
        "git flow hotfix finish reload\n",
        "tag description: hotfix/v0.0.2\n",
        "```\n",
        "\n",
        "**Important:** Use hotfixes sparingly and only for critical issues in the production environment.\n",
        "\n",
        "### Useful Commands\n",
        "\n",
        "* To initialize a Gitflow workflow:\n",
        "\n",
        "```\n",
        "git flow init\n",
        "```\n",
        "\n",
        "* To delete a tag:\n",
        "\n",
        "```\n",
        "git tag -d 0.0.1\n",
        "```\n",
        "\n",
        "* To list tags:\n",
        "\n",
        "```\n",
        "git tag --list\n",
        "```\n",
        "\n",
        "* To list branches (local and remote):\n",
        "\n",
        "```\n",
        "git branch -a\n",
        "```\n",
        "\n",
        "* To delete a local branch:\n",
        "\n",
        "```\n",
        "git branch -D release/0.0.1\n",
        "```\n",
        "\n",
        "* To abort a release (avoid if possible):\n",
        "\n",
        "```\n",
        "git flow release delete -f v0.0.3\n",
        "```\n",
        "\n",
        "* To show Gitflow configuration:\n",
        "\n",
        "```\n",
        "git config --get-regexp '^gitflow\\..*'\n",
        "```\n",
        "\n",
        "* To delete Gitflow configuration:\n",
        "\n",
        "```\n",
        "git config --unset gitflow.feature.finish.keep\n",
        "```\n",
        "\n",
        "* To pull all remote branches into your local repository:\n",
        "\n",
        "```\n",
        "git pull --all\n",
        "```\n",
        "\n",
        "### Skills\n",
        "\n",
        "* Acheived\n",
        " * Release process\n",
        " * Postman POST request preparation\n",
        " * Unit tests\n",
        " * Clean code\n",
        " * Documentation\n",
        " * IA support: Gemini (focused on Google) and ChatGPT (for coding)\n",
        " * Self and third assumptions\n",
        " * Avoid unnecessary disk/memory/cpu processing\n",
        " * I/O\n",
        " * Google Cloud (Storage, BigQuery, IAM)\n",
        " * CTE and SQL (RegExp, Unnest)\n",
        " * GIT skills (good practices and git flow practices)\n",
        "   * git-flow\n",
        "   * git practices\n",
        "   * git network\n",
        "   * pull requests\n",
        "   * hotfix\n",
        "   * release\n",
        "   * feature\n",
        "   * git commands\n",
        "\n",
        "* Partially achieved\n",
        " * Logging to Python files (there are various py files without logging)\n",
        " * File organization (acheived at the end instead of the beginning. Besides, I miss a better folder structure for the project)\n",
        " * TDD (I didn't save my tests in Google Colab Notebook, that is how I acheived the code as it was new technologies for me, my strength was with Spark and Databricks)\n",
        " * Error handling (I tried to consider error handling and board cases. Focusing especifically in inputs and outputs)\n",
        " * Type Annotations (This was very helpful for me to understand what type of data I receive or return, very useful for understanding how Google Cloud libraries work. I had to look at the code of a couple of them to understand them.)\n",
        " * Functional Programming (First-class functions, pure functions, and function composition. I am interested in learning Cats for Scala.)\n",
        " * Decorators\n",
        " * Time and memory measurement\n",
        "\n",
        "* Not acheived\n",
        " * Documented Improvements (There is a lot that can be improved but I did my best)\n",
        " * GitHub Actions (I am really interested on this to automate the manual jobs that I had to do on this challenge)\n",
        " * Unit Test (I didn't have the time to test with unit tests in my IDE, I only created them with some tests to see if I tested well my code when developing)\n",
        "\n",
        "### Insights from ChatGPT\n",
        "In your code, you've incorporated several practices and techniques related to Python programming and interaction with the Google Cloud Platform:\n",
        "\n",
        "1. **Logging:** Utilizing the `logging` module to record events and informative messages throughout function execution.\n",
        "\n",
        "2. **Type Annotations:** Employing type hints (`typing`) to specify parameter and return value types in functions, enhancing code readability and error detection.\n",
        "\n",
        "3. **Exception Handling:** Implementing error management to address potential issues during authentication, dataset and table creation, and data loading to BigQuery, enhancing code robustness.\n",
        "\n",
        "4. **Google Cloud API Usage:** Interacting with the BigQuery API within the Google Cloud Platform to execute operations like authentication, dataset and table creation, and data loading.\n",
        "\n",
        "5. **Functional Programming:** While not explicitly applying functional programming design patterns, you adhere to certain functional programming principles by treating functions as first-class citizens and maintaining purity by minimizing side effects.\n",
        "\n",
        "6. **Dependency Injection:** Passing the BigQuery client as a parameter to functions requiring it, following the dependency injection principle, thereby promoting modularity and code reusability.\n",
        "\n",
        "7. **Adherence to Good Development Practices:** Demonstrating good practices such as using descriptive function and variable names, implementing explicit error handling, and organizing code into specialized functions.\n",
        "\n",
        "In summary, your code exhibits a structured and meticulous approach to interfacing with the BigQuery API, leveraging Python-specific features and adhering to sound development principles.\n",
        "\n",
        "**Areas of improvement:**\n",
        "Here are some areas where you could consider improvements or refinements in your code:\n",
        "\n",
        "1. **Error Handling:** While you have implemented error handling, consider adding more specific error messages or logging details to aid in troubleshooting.\n",
        "\n",
        "2. **Unit Testing:** You've mentioned an interest in unit testing; consider implementing unit tests for your functions to ensure their correctness and robustness.\n",
        "\n",
        "3. **Code Duplication:** There are some repetitive patterns in your code, such as logging messages and dataset/table existence checks. Consider refactoring these into reusable functions or decorators to adhere more closely to the DRY (Don't Repeat Yourself) principle.\n",
        "\n",
        "4. **Function Size:** Some of your functions could be broken down into smaller, more focused functions. This can improve readability and maintainability.\n",
        "\n",
        "5. **Documentation:** While you have docstrings for your functions, consider adding more detailed explanations, especially regarding the purpose and behavior of each function.\n",
        "\n",
        "6. **Configuration Management:** Externalize configuration settings (e.g., project ID, dataset names) to separate configuration files or environment variables for better manageability.\n",
        "\n",
        "7. **Consistent Logging:** Ensure consistency in logging levels and messages throughout your codebase for easier debugging and monitoring.\n",
        "\n",
        "8. **Data Validation:** Consider adding more robust data validation, especially when loading data from external sources, to ensure data integrity and prevent potential issues.\n",
        "\n",
        "9. **Code Comments:** While your code is generally well-structured and readable, adding comments to explain complex logic or decision-making processes can further improve readability for others (and your future self).\n",
        "\n",
        "10. **Security Considerations:** Ensure that your code follows best practices for handling sensitive information, such as project IDs and authentication credentials, to prevent accidental exposure.\n",
        "\n",
        "By addressing these areas, you can enhance the readability, maintainability, and reliability of your codebase.\n",
        "\n",
        "### Insights from Gemini\n",
        "Based on the code snippets you've shown, here's what I can glean about your potential programming interests and skills:\n",
        "\n",
        "**Areas of Interest:**\n",
        "\n",
        "- **Python:** You seem comfortable working with Python code, utilizing functions, type hints, docstrings, and iterating through data structures.\n",
        "- **Clean Code Principles:** You're interested in writing clean and well-structured code, as evidenced by your desire to improve code readability and maintainability.\n",
        "- **Data Analysis:** The `q1_time` function suggests you might be working with BigQuery for data analysis tasks.\n",
        "- **Environment Variables:** The code snippet using `os.environ` indicates an interest in accessing and potentially using system environment variables.\n",
        "- **Time Measurement:** The `measure_elapsed_time` function demonstrates an understanding of measuring elapsed time in Python.\n",
        "\n",
        "**Overall Skills:**\n",
        "\n",
        "- **Code Comprehension:** You can effectively understand and explain the purpose and functionality of code snippets.\n",
        "- **Code Improvement:** You're interested in applying clean code principles to improve code quality.\n",
        "- **Problem-Solving:** You can break down code into its components and analyze its functionality.\n",
        "\n",
        "**Additional Considerations:**\n",
        "\n",
        "- The specific details of the code snippets (like `q1_time`) might hint at a larger project you're working on, but without more context, it's difficult to say for sure.\n",
        "- The lack of code execution (due to my safety restrictions) prevents me from observing how the code interacts with external systems or libraries.\n",
        "\n",
        "**Areas for Improvement:**\n",
        "\n",
        "## Areas for Improvement Based on Your Code:\n",
        "\n",
        "**1. Handling Errors and Exceptions:**\n",
        "\n",
        "- While the `q1_time` function includes basic error handling with `try...except`, consider being more specific about the exceptions you might encounter (e.g., `bigquery.QueryJobClientError`). This allows for more tailored error messages and handling.\n",
        "\n",
        "**2. Testing:**\n",
        "\n",
        "The provided snippets don't explicitly show unit tests. Implementing unit tests for your functions would ensure their correctness and reliability as your code evolves.\n",
        "\n",
        "**3. Logging:**\n",
        "\n",
        "While the code prints information to the console, consider using a proper logging library like `logging` for a more structured and centralized approach. This helps track code execution and identify potential issues more easily.\n",
        "\n",
        "**4. Modularity:**\n",
        "\n",
        "The `q1_time` function is a good example of modularity. Explore further opportunities to break down larger code blocks into smaller, reusable functions with clear responsibilities.\n",
        "\n",
        "**5. Readability:**\n",
        "\n",
        "- **Variable Names:** While variable names like `key` and `value` are generally understandable, consider using more descriptive names when appropriate. For example, in `q1_time`, `date_field` and `string_field` could be more specific based on the actual data.\n",
        "- **Comments:** Add comments to explain complex logic or non-obvious parts of the code, especially when dealing with external libraries or specific data structures.\n",
        "\n",
        "**6. Specific Libraries/Frameworks:**\n",
        "\n",
        "- The `q1_time` function utilizes the BigQuery client library. Consider exploring more advanced features of the library for data manipulation or transformation within BigQuery itself (if needed).\n",
        "\n",
        "**Additional Tips:**\n",
        "\n",
        "- **Code Formatting:** Consistent code formatting using tools like `autopep8` improves readability.\n",
        "- **Static Code Analysis:** Utilize static code analysis tools like `pylint` or `mypy` to identify potential issues early on.\n",
        "\n",
        "Remember, clean code is an ongoing process. By continuously striving for improvement in these areas, you'll write code that is not only functional but also maintainable, readable, and robust."
      ],
      "metadata": {
        "id": "fxH6G_wWmBaQ"
      }
    }
  ]
}